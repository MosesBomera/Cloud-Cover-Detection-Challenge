{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use Deep Learning, PyTorch Lightning, and the Planetary Computer to Predict Cloud Cover in Satellite Imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure><center><img src=\"https://drivendata-public-assets.s3.amazonaws.com/clouds-use-volcano.jpeg\" width=\"350\" height=\"40\"></center></figure>\n",
    "\n",
    "<p><center><i>Sentinel-2 satellite imagery being used to track a volcano eruption on La Palma, Spain. Credit: <a href=\"https://news.sky.com/story/la-palma-volcano-new-satellite-images-show-violent-eruption-from-space-as-lava-flows-across-island-12431412\">Sky News</a></i></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the benchmark notebook for the On Cloud N: Cloud Cover Detection Challenge! If you are just getting started, check out competition [homepage](https://www.drivendata.org/competitions/83/cloud-cover/page/396/). The goal of this benchmark is to:\n",
    "\n",
    "1. Demonstrate how to explore and work with the data\n",
    "2. Provide a basic framework for building a model\n",
    "3. Demonstrate how to package your work correctly for submission\n",
    "\n",
    "This notebook is a version of the benchmark [blog post](https://www.drivendata.co/blog/cloud-cover-benchmark/) posted on the competition website, with some small tweaks to run directly in the planetary computer hub.\n",
    "\n",
    "You can either expand on and improve this benchmark, or start with something completely different! Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Satellite imagery is critical for a wide variety of applications from disaster management and recovery, to agriculture, to military intelligence. Clouds present a major obstacle for all of these use cases, and usually have to be identified and removed from a dataset before satellite imagery can be used. Improving methods of identifying clouds can unlock the potential of an unlimited range of satellite imagery use cases, enabling faster, more efficient, and more accurate image-based research.\n",
    "\n",
    "**In this challenge, your goal is to detect cloud cover in satellite imagery**. \n",
    "\n",
    "The challenge uses publicly available satellite data from the [Sentinel-2 mission](https://sentinel.esa.int/web/sentinel/missions/sentinel-2), which captures wide-swath, high-resolution, multi-spectral imaging. For each tile, data is separated into 13 spectral bands capturing the full visible spectrum, near-infrared, and infrared light. Feature data is shared through Microsoft's [Planetary Computer](https://planetarycomputer.microsoft.com/).\n",
    "\n",
    "The ground truth labels for cloud cover are part of a new dataset created by [Radiant Earth Foundation](https://www.radiant.earth/). After the competition ends, the labels dataset will be made publicly available to contribute to ongoing learning and advancement in the field.\n",
    "\n",
    "In this post, we'll cover:\n",
    "\n",
    "- [Exploring the data](#explore-the-data)\n",
    "- [Preparing the data](#split-data)\n",
    "- [Following the code execution submission format](#code-exec)\n",
    "- [Building the benchmark model](#build-model)\n",
    "- [Fitting the benchmark model](#fit-model)\n",
    "- [Generating a code execution submission](#generate-submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas_path pytorch_lightning cloudpathlib loguru typer\n",
    "!pip install --upgrade pandas==1.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_path import path  # noqa\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"explore-the-data\"></a>\n",
    "\n",
    "### Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition data is already available within the Planetary Computer Hub in a read-only attached volume. It can be accessed at `/driven-data/cloud-cover`.\n",
    "\n",
    "If you are working outside of the Hub, you'll need to download the data to a local folder per the instructions on the competition's data download [page](https://www.drivendata.org/competitions/83/cloud-cover/data/). You'll need to join the competition in order to view the data page. \n",
    "\n",
    "Data directory contents:\n",
    "```\n",
    ".\n",
    "├── train_features\n",
    "│   ├── train_chip_id_1\n",
    "│   │   ├── B02.tif\n",
    "│   │   ├── B03.tif\n",
    "│   │   ├── B04.tif\n",
    "│   │   └── B08.tif\n",
    "│   └── ... \n",
    "├── train_metadata.csv\n",
    "└── train_labels\n",
    "    ├── train_chip_id_1.tif\n",
    "    └── ... \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/driven-data/cloud-cover\")\n",
    "TRAIN_FEATURES = DATA_DIR / \"train_features\"\n",
    "TRAIN_LABELS = DATA_DIR / \"train_labels\"\n",
    "\n",
    "assert TRAIN_FEATURES.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data consists of 11,748 \"chips\". Each chip is imagery of a specific area captured at a specific point in time. There are four images associated with each chip in the competition data. Each image within a chip captures light from a different range of wavelengths, or \"band\". For example, the B02 band for each chip shows the strengh of visible blue light, which has a wavelength around 492 nanometers (nm). The bands provided are:\n",
    "\n",
    "\n",
    "<table border=\"1\" class=\"table\" style=\"width:70%; margin-left:auto; margin-right:auto\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>Band</th>\n",
    "      <th>Description</th>\n",
    "      <th>Center wavelength</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>B02</td>\n",
    "      <td>Blue visible light</td>\n",
    "      <td>497 nm</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>B03</td>\n",
    "      <td>Green visible light</td>\n",
    "      <td>560 nm</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>B04</td>\n",
    "      <td>Red visible light</td>\n",
    "      <td>665 nm</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>B08</td>\n",
    "      <td>Near infrared light</td>\n",
    "      <td>835 nm</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Let's set a global variable with the list of bands in our training data, so it's easier to iterate over them for future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS = [\"B02\", \"B03\", \"B04\", \"B08\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the metadata for the train and test sets, to understand what the images in this competition capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = pd.read_csv(DATA_DIR / \"train_metadata.csv\")\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many different chip ids, locations, and datetimes are there?\n",
    "train_meta[[\"chip_id\", \"location\", \"datetime\"]].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one row per chip, and each `chip_id` is unique. There are columns for:\n",
    "\n",
    "- `location`: General location of the chip\n",
    "\n",
    "- `datetime`: Date and time the satellite image was captured\n",
    "\n",
    "- `cloudpath`: All of the satellite images for this competition are hosted on Azure Blob Storage containers. You can use this `cloudpath`, combined with the provided `download_data.py` script, to download any single chip from the container using the [cloudpathlib](https://cloudpathlib.drivendata.org/stable/) library. You can also download all of the data at once, rather than chip by chip. See the `data_download_instructions.txt` file attached to the Data Download [page](https://www.drivendata.org/competitions/83/cloud-cover/data/) for details.\n",
    "\n",
    "Let's take a look at the distribution of chips by location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location_counts = (\n",
    "    train_meta.groupby(\"location\")[\"chip_id\"].nunique().sort_values(ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "train_location_counts.head(25).plot(kind=\"bar\", color=\"lightgray\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Location\")\n",
    "plt.ylabel(\"Number of Chips\")\n",
    "plt.title(\"Number of Train Chips by Location (Top 25)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test images are from all over the world! Location names can be countries (eg. Eswatini), cities (eg. Lusaka), or broader regions of a country (eg. Australia - Central). [Sentinel-2](https://sentinel.esa.int/web/sentinel/missions/sentinel-2/observation-scenario) flies over the part of the Earth between 56° South and 82.8° North, between Cape Horn and slightly north of Greenland, so our observations are all between these two latitudes. The chips are mostly in Africa and South America, with some in Australia too.\n",
    "\n",
    "We also have a timestamp for each chip. What is the time range in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta[\"datetime\"] = pd.to_datetime(train_meta[\"datetime\"])\n",
    "train_meta[\"year\"] = train_meta.datetime.dt.year\n",
    "train_meta.groupby(\"year\")[[\"chip_id\"]].nunique().sort_index().rename(\n",
    "    columns={\"chip_id\": \"chip_count\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta[\"datetime\"].min(), train_meta[\"datetime\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chips_per_locationtime = (\n",
    "    train_meta.groupby([\"location\", \"datetime\"])[[\"chip_id\"]]\n",
    "    .nunique()\n",
    "    .sort_values(by=\"chip_id\", ascending=False)\n",
    "    .rename(columns={\"chip_id\": \"chip_count\"})\n",
    ")\n",
    "chips_per_locationtime.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the images in the data were captured between February of 2018 and September of 2020. We can also see from the training data that many chips share the same location and time. \n",
    "\n",
    "We know from the [problem description](https://www.drivendata.org/competitions/83/cloud-cover/page/398/#metadata) that all of the locations in the test set are new, and there is no overlap between locations in the train set and the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images\n",
    "\n",
    "Next, let's explore the actual satellite images - the star of the show for this challenge!\n",
    "\n",
    "For convenience, let's first add the paths to all of the feature images per chip. Remember, the folder for each chip has four images, each of which corresponds to a different band of light wavelengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_paths(df, feature_dir, label_dir=None, bands=BANDS):\n",
    "    \"\"\"\n",
    "    Given dataframe with a column for chip_id, returns a dataframe with a column\n",
    "    added indicating the path to each band's TIF image as \"{band}_path\", eg \"B02_path\".\n",
    "    A column is also added to the dataframe with paths to the label TIF, if the\n",
    "    path to the labels directory is provided.\n",
    "    \"\"\"\n",
    "    for band in bands:\n",
    "        df[f\"{band}_path\"] = feature_dir / df[\"chip_id\"] / f\"{band}.tif\"\n",
    "        # make sure a random sample of paths exist\n",
    "        assert df.sample(n=40, random_state=5)[f\"{band}_path\"].path.exists().all()\n",
    "    if label_dir is not None:\n",
    "        df[\"label_path\"] = label_dir / (df[\"chip_id\"] + \".tif\")\n",
    "        # make sure a random sample of paths exist\n",
    "        assert df.sample(n=40, random_state=5)[\"label_path\"].path.exists().all()\n",
    "    return df\n",
    "\n",
    "train_meta = add_paths(train_meta, TRAIN_FEATURES, TRAIN_LABELS)\n",
    "train_meta.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is a *GeoTIFF*, a raster image file that contains geographic metadata. This metadata can include coordinates, an affine transform, and a coordinate reference system (CRS) projection. The package [rasterio](https://rasterio.readthedocs.io/en/latest/) makes it easy to interact with our geospatial raster data.\n",
    "\n",
    "Lets look at the red visible band (B04) image for a random chip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_chip = train_meta[train_meta[\"chip_id\"] == \"pbyl\"].iloc[0]\n",
    "example_chip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example chip is an image taken from Lodwar, a northwestern town in Kenya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(example_chip[\"B04_path\"]) as img:\n",
    "    chip_metadata = img.meta\n",
    "    img_array = img.read(1)\n",
    "chip_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the features are single-band images, with a shape of 512 x 512. The pixel values for each image measure the strength of light reflected back to the satellite for the specific set of wavelengths in that band. The array below shows the strength of red visible light, with wavelengts around 665 nm.\n",
    "\n",
    "We can also see that there are no missing values in the image. This should be the case for all of the provided competition feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the image array look like?\n",
    "print(\"Image array shape:\", img_array.shape)\n",
    "img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(img_array).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_array)\n",
    "plt.title(f\"B04 band for chip id {example_chip.chip_id}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Coordinates\n",
    "\n",
    "Using the metadata returned by `rasterio`, we can also get longitude and latitude coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longitude/latitude of image's center\n",
    "with rasterio.open(example_chip[\"B04_path\"]) as img:\n",
    "    lon, lat = img.lnglat()\n",
    "    bounds = img.bounds\n",
    "print(f\"Longitude: {lon}, latitude: {lat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the longitude and latitude of the center of the image, but the bounding box values look very different. That's because the bounding box is given in whatever coordinate reference system the image is projected in, rather than traditional longitude and latiude. We can see which system with the `crs` value from the metadata.\n",
    "\n",
    "We can convert the bounding box to longitude and latitude using `pyproj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_long_bounds(filepath):\n",
    "    \"\"\"Given the path to a GeoTIFF, returns the image bounds in latitude and\n",
    "    longitude coordinates.\n",
    "\n",
    "    Returns points as a tuple of (left, bottom, right, top)\n",
    "    \"\"\"\n",
    "    with rasterio.open(filepath) as im:\n",
    "        bounds = im.bounds\n",
    "        meta = im.meta\n",
    "    # create a converter starting with the current projection\n",
    "    current_crs = pyproj.CRS(meta[\"crs\"])\n",
    "    crs_transform = pyproj.Transformer.from_crs(current_crs, current_crs.geodetic_crs)\n",
    "\n",
    "    # returns left, bottom, right, top\n",
    "    return crs_transform.transform_bounds(*bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left, bottom, right, top = lat_long_bounds(example_chip[\"B04_path\"])\n",
    "print(\n",
    "    f\"Image coordinates (lat, long):\\nStart: ({left}, {bottom})\"\n",
    "    f\"\\nEnd: ({right}, {top})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True color image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a composite image from the three visible bands (blue, green, and red) to visualize a high-quality, true color image. To show the true color image, we'll use the `rioxarray` and `xrspatial` packages designed for Sentinel-2 satellite data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import xrspatial.multispectral as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_color_img(chip_id, data_dir=TRAIN_FEATURES):\n",
    "    \"\"\"Given the path to the directory of Sentinel-2 chip feature images,\n",
    "    plots the true color image\"\"\"\n",
    "    chip_dir = data_dir / chip_id\n",
    "    red = rioxarray.open_rasterio(chip_dir / \"B04.tif\").squeeze()\n",
    "    green = rioxarray.open_rasterio(chip_dir / \"B03.tif\").squeeze()\n",
    "    blue = rioxarray.open_rasterio(chip_dir / \"B02.tif\").squeeze()\n",
    "\n",
    "    return ms.true_color(r=red, g=green, b=blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "im = true_color_img(example_chip.chip_id)\n",
    "ax.imshow(im)\n",
    "plt.title(f\"True color image for chip id {example_chip.chip_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty high quality image!\n",
    "\n",
    "Let's look at a few random training chips and their cloud labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random_chip(random_state):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    random_chip = train_meta.sample(random_state=random_state).iloc[0]\n",
    "\n",
    "    ax[0].imshow(true_color_img(random_chip.chip_id))\n",
    "    ax[0].set_title(f\"Chip {random_chip.chip_id}\\n(Location: {random_chip.location})\")\n",
    "    label_im = Image.open(random_chip.label_path)\n",
    "    ax[1].imshow(label_im)\n",
    "    ax[1].set_title(f\"Chip {random_chip.chip_id} label\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_chip(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_chip(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_chip(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the example chips, we can see that there is a very big variation in the amount of cloud cover per chip. The chip from Riobamba, Ecuador is almost completely obscured by clouds, while the chip from Macapá, Brazil has almost none."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='split-data'></a>\n",
    "\n",
    "### Split the data\n",
    "\n",
    "To train our model, we want to separate the data into a \"training\" set and a \"validation\" set. That way we'll have a portion of labelled data that was not used in model training, which can give us a more accurate sense of how our model will perform on the competition test data. Remember, none of the test set locations are in competition training data, so your model's will performance will ultimately be measured on unseen locations.\n",
    "\n",
    "We have chosen the simplest route, and split our training chips randomly into 1/3 validation and 2/3 training. You may want to think about splitting by location instead of by chip, to better check how your model will do in new settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(9)  # set a seed for reproducibility\n",
    "\n",
    "# put 1/3 of chips into the validation set\n",
    "chip_ids = train_meta.chip_id.unique().tolist()\n",
    "val_chip_ids = random.sample(chip_ids, round(len(chip_ids) * 0.33))\n",
    "\n",
    "val_mask = train_meta.chip_id.isin(val_chip_ids)\n",
    "val = train_meta[val_mask].copy().reset_index(drop=True)\n",
    "train = train_meta[~val_mask].copy().reset_index(drop=True)\n",
    "\n",
    "val.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features from labels\n",
    "feature_cols = [\"chip_id\"] + [f\"{band}_path\" for band in BANDS]\n",
    "\n",
    "val_x = val[feature_cols].copy()\n",
    "val_y = val[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "train_x = train[feature_cols].copy()\n",
    "train_y = train[[\"chip_id\", \"label_path\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='code-exec'></a>\n",
    "\n",
    "### Code Execution Format\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<strong>This is a code submission challenge! Rather than submitting your predicted labels, you'll package everything needed to do inference and submit that for containerized execution.</strong> The <a href=\"https://github.com/drivendataorg/cloud-cover-runtime\">runtime repository</a> contains the complete specification for the runtime. See the Code Submission Format <a href=\"https://www.drivendata.org/competitions/83/cloud-cover/page/412/\">page</a> for full details. You can learn more about how our code execution competitions work <a href=\"https://www.drivendata.co/blog/code-execution-competitions/\">here</a>.\n",
    "</div>\n",
    "\n",
    "Rather than just generating predictions, this benchmark will show you how to create a correct submission for code execution. Your final submission will be a zipped archive called `submission.zip`. We'll create a folder called `benchmark_src`, and develop a complete code execution submission in that folder. `benchmark_src` will ultimately include:\n",
    "\n",
    "1. The weights for our trained model\n",
    "\n",
    "2. A `main.py` script that loads the model, performs inference on images stored in `data/test_features`, generates chip-level predictions, and saves them out to `predictions/`. File names should match the chip IDs from the test dataset. For example, if the test set includes a chip with ID `abcd`, running `main.py` must write out a predicted cloud cover TIF mask to `predictions/abcd.tif`. The predictions should be .tifs with values of `1` and `0` indicating cloud or no cloud.\n",
    "\n",
    "3. Any additional scripts that `main.py` needs in order to run\n",
    "\n",
    "`main.py` must be able to run without getting any input arguments, so make sure you set all your defaults so that calling `python main.py` alone runs the full process. **Your final submission should *not* include prediction files.**\n",
    "\n",
    "All of the key code for inference is written out to scripts in `benchmark_src` using the line magic `%%file` instead of being run directly in this notebook. The final scripts that are written out by this notebook are also saved in a folder of the [runtime repo](https://github.com/drivendataorg/cloud-cover-runtime/tree/main/benchmark_src)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create benchmark_src folder\n",
    "submission_dir = Path(\"benchmark_src\")\n",
    "if submission_dir.exists():\n",
    "    shutil.rmtree(submission_dir)\n",
    "\n",
    "submission_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run inference in this notebook by importing classes and functions from the scripts in `benchmark_src`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build-model\"></a>\n",
    "\n",
    "### Build the model\n",
    "\n",
    "As a starting point, we'll build a basic neural network model using some of the standard libraries. That way there's lots of room for you to make improvements!\n",
    "\n",
    "We'll use [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/), a package that introduces useful conventions for training deep learning models on top of PyTorch. Our model will start with a publicly available [convolutional neural network](https://www.youtube.com/watch?v=aircAruvnKk&t=995s) called [U-Net](https://arxiv.org/abs/1505.04597) that is pretrained for [semantic segmentation](https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47).\n",
    "\n",
    "**U-Net** was first designed to help process biomedical imaging and identify things like signs of disease. The basic structure is an encoder network followed by a decoder network. We'll use a pretrained backbone called [ResNet34](https://arxiv.org/abs/1512.03385) as our encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CloudDataset`\n",
    "\n",
    "The first step of building the model is actually writing a class that will tell the model how to handle our feature data. Thankfully, the PyTorch `Dataset` and `DataLoader` classes take care of most of this for us! A `Dataset` object allows us to define custom methods for working with the data, and a `DataLoader` object parallelizes data loading. If you haven't worked with these classes before, we highly recommend this short [tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "We'll write a custom dataset called `CloudDataset` that reads our satellite image training data into memory, converts the data to PyTorch tensors, and serves the data to our model in batches. Our class will inherit from `torch.utils.data.Dataset`, and [override](https://www.tutorialspoint.com/overriding-methods-in-python) a few key methods:\n",
    "\n",
    "`__len__()`: return the number of chips in the dataset\n",
    "\n",
    "`__getitem__()`: return a sample from the dataset as a dictionary, with keys for:\n",
    "- `chip_id`: unique identifiers for each chip\n",
    "\n",
    "- `chip`: the image array of the chip, with all four provided bands combined. The shape of each image array will be (4, 512, 512). \n",
    "\n",
    "- `label`: the label tif mask, if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file {submission_dir}/cloud_dataset.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "\n",
    "\n",
    "class CloudDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Reads in images, transforms pixel values, and serves a\n",
    "    dictionary containing chip ids, image tensors, and\n",
    "    label masks (where available).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_paths: pd.DataFrame,\n",
    "        bands: List[str],\n",
    "        y_paths: Optional[pd.DataFrame] = None,\n",
    "        transforms: Optional[list] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudDataset class.\n",
    "\n",
    "        Args:\n",
    "            x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands\n",
    "            bands (list[str]): list of the bands included in the data\n",
    "            y_paths (pd.DataFrame, optional): a dataframe with a for each chip and columns for chip_id\n",
    "                and the path to the label TIF with ground truth cloud cover\n",
    "            transforms (list, optional): list of transforms to apply to the feature data (eg augmentations)\n",
    "        \"\"\"\n",
    "        self.data = x_paths\n",
    "        self.label = y_paths\n",
    "        self.transforms = transforms\n",
    "        self.bands = bands\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Loads an n-channel image from a chip-level dataframe\n",
    "        img = self.data.loc[idx]\n",
    "        band_arrs = []\n",
    "        for band in self.bands:\n",
    "            with rasterio.open(img[f\"{band}_path\"]) as b:\n",
    "                band_arr = b.read(1).astype(\"float32\")\n",
    "            band_arrs.append(band_arr)\n",
    "        x_arr = np.stack(band_arrs, axis=-1)\n",
    "\n",
    "        # Apply data augmentations, if provided\n",
    "        if self.transforms:\n",
    "            x_arr = self.transforms(image=x_arr)[\"image\"]\n",
    "        x_arr = np.transpose(x_arr, [2, 0, 1])\n",
    "\n",
    "        # Prepare dictionary for item\n",
    "        item = {\"chip_id\": img.chip_id, \"chip\": x_arr}\n",
    "\n",
    "        # Load label if available\n",
    "        if self.label is not None:\n",
    "            label_path = self.label.loc[idx].label_path\n",
    "            with rasterio.open(label_path) as lp:\n",
    "                y_arr = lp.read(1).astype(\"float32\")\n",
    "            # Apply same data augmentations to the label\n",
    "            if self.transforms:\n",
    "                y_arr = self.transforms(image=y_arr)[\"image\"]\n",
    "            item[\"label\"] = y_arr\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom dataset does not implement any transformations or feature engineering. Applying data augmentations and transformations is a good way to train your model on a wider variety of scenarios without needing more data. The [albumentations](https://albumentations.ai/) library is a good place to start if you want to implement some of these!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss class\n",
    "\n",
    "Next we'll set up a custom loss class for calculating intersection over union (IOU), the competition [performance metric](https://www.drivendata.org/competitions/83/cloud-cover/page/398/#performance-metric). During training, we'll define the best model as the one that gets the highest IOU on the validation set.\n",
    "\n",
    "For our training steps, we'll minimize [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression). Cross-entropy loss broadly evaluates the differences between predicted and ground truth pixels and averages over all pixels. For this, we can use the built-in `torch.nn.CrossEntropyLoss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file {submission_dir}/losses.py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def intersection_over_union(pred, true):\n",
    "    \"\"\"\n",
    "    Calculates intersection and union for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): a tensor of predictions\n",
    "        true (torc.Tensor): a tensor of labels\n",
    "\n",
    "    Returns:\n",
    "        intersection (int): total intersection of pixels\n",
    "        union (int): total union of pixels\n",
    "    \"\"\"\n",
    "    valid_pixel_mask = true.ne(255)  # valid pixel mask\n",
    "    true = true.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "    pred = pred.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "\n",
    "    # Intersection and union totals\n",
    "    intersection = np.logical_and(true, pred)\n",
    "    union = np.logical_or(true, pred)\n",
    "    return intersection.sum() / union.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CloudModel`\n",
    "\n",
    "Now is the moment we've all been waiting for - coding our actual model! \n",
    "\n",
    "Again, we'll make our lives simpler by starting with the [`pl.LightningModule`](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html) class from Pytorch Lightning. This comes with most of the logic we need, so we only have to specify components that are specific to our modeling setup. Our custom `CloudModel` class will define:\n",
    "\n",
    "- `__init__`: how to instantiate a `CloudModel` class\n",
    "\n",
    "- `forward`: forward pass for an image in the neural network propogation\n",
    "\n",
    "- `training_step`: switch the model to train mode, implement the forward pass, and calculate training loss (cross-entropy) for a batch\n",
    "\n",
    "- `validation_step`: switch the model to eval mode and calculate validation loss (IOU) for the batch\n",
    "\n",
    "- `train_dataloader`: call an iterable over the training dataset for automatic batching\n",
    "\n",
    "- `val_dataloader`: call an iterable over the validation dataset for automatic batching\n",
    "\n",
    "- `configure_optimizers`: configure an [optimizer](https://pytorch.org/docs/stable/optim.html) and a [scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html) to dynamically adjust the learning rate based on the number of epochs\n",
    "\n",
    "- `_prepare_model`: load the U-Net model with a ResNet34 backbone from the `segmentation_models_pytorch` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file {submission_dir}/cloud_model.py\n",
    "from typing import Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from losses import intersection_over_union\n",
    "except ImportError:\n",
    "    from benchmark_src.cloud_dataset import CloudDataset\n",
    "    from benchmark_src.losses import intersection_over_union\n",
    "\n",
    "\n",
    "class CloudModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bands: List[str],\n",
    "        x_train: Optional[pd.DataFrame] = None,\n",
    "        y_train: Optional[pd.DataFrame] = None,\n",
    "        x_val: Optional[pd.DataFrame] = None,\n",
    "        y_val: Optional[pd.DataFrame] = None,\n",
    "        hparams: dict = {},\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudModel class based on the pl.LightningModule\n",
    "        (https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\n",
    "\n",
    "        Args:\n",
    "            bands (list[str]): Names of the bands provided for each chip\n",
    "            x_train (pd.DataFrame, optional): a dataframe of the training features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_train (pd.DataFrame, optional): a dataframe of the training labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            x_val (pd.DataFrame, optional): a dataframe of the validation features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_val (pd.DataFrame, optional): a dataframe of the validation labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            hparams (dict, optional): Dictionary of additional modeling parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # required\n",
    "        self.bands = bands\n",
    "\n",
    "        # optional modeling params\n",
    "        self.backbone = self.hparams.get(\"backbone\", \"resnet34\")\n",
    "        self.weights = self.hparams.get(\"weights\", \"imagenet\")\n",
    "        self.learning_rate = self.hparams.get(\"lr\", 1e-3)\n",
    "        self.patience = self.hparams.get(\"patience\", 4)\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 2)\n",
    "        self.batch_size = self.hparams.get(\"batch_size\", 32)\n",
    "        self.gpu = self.hparams.get(\"gpu\", False)\n",
    "        self.transform = None\n",
    "\n",
    "        # Instantiate datasets, model, and trainer params if provided\n",
    "        self.train_dataset = CloudDataset(\n",
    "            x_paths=x_train,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_train,\n",
    "            transforms=self.transform,\n",
    "        )\n",
    "        self.val_dataset = CloudDataset(\n",
    "            x_paths=x_val,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_val,\n",
    "            transforms=None,\n",
    "        )\n",
    "        self.model = self._prepare_model()\n",
    "\n",
    "    ## Required LightningModule methods ##\n",
    "\n",
    "    def forward(self, image: torch.Tensor):\n",
    "        # Forward pass\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.train_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_train and y_train must be specified when CloudModel is instantiated to run training\"\n",
    "            )\n",
    "\n",
    "        # Switch on training mode\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = self.forward(x)\n",
    "\n",
    "        # Log batch loss\n",
    "        loss = torch.nn.CrossEntropyLoss(reduction=\"none\")(preds, y).mean()\n",
    "        self.log(\n",
    "            \"loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.val_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_val and y_val must be specified when CloudModel is instantiated to run validation\"\n",
    "            )\n",
    "\n",
    "        # Switch on validation mode\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass & softmax\n",
    "        preds = self.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5) * 1  # convert to int\n",
    "\n",
    "        # Log batch IOU\n",
    "        batch_iou = intersection_over_union(preds, y)\n",
    "        self.log(\n",
    "            \"iou\", batch_iou, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return batch_iou\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # DataLoader class for training\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # DataLoader class for validation\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "        return [opt], [sch]\n",
    "\n",
    "    ## Convenience Methods ##\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        # Instantiate U-Net model\n",
    "        unet_model = smp.Unet(\n",
    "            encoder_name=self.backbone,\n",
    "            encoder_weights=self.weights,\n",
    "            in_channels=4,\n",
    "            classes=2,\n",
    "        )\n",
    "        if self.gpu:\n",
    "            unet_model.cuda()\n",
    "\n",
    "        return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_src.cloud_model import CloudModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fit-model'></a>\n",
    "\n",
    "### Fit the model\n",
    "\n",
    "It's go time! The only required arguments to fit the model are the train features, the train labels, the validation features, and the validation labels. \n",
    "\n",
    "We've opted to train with all of the defaults hyperparameters defined in `__init__` for simplicity, so there is a lot of room for tweaks and improvements! You can experiment with things like learning rate, patience, batch size, and much much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pytorch_lightning.Trainer object\n",
    "cloud_model = CloudModel(\n",
    "    bands=BANDS,\n",
    "    x_train=train_x,\n",
    "    y_train=train_y,\n",
    "    x_val=val_x,\n",
    "    y_val=val_y,\n",
    "    hparams={\"num_workers\": 7, \"batch_size\": 8},\n",
    ")\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"iou_epoch\", mode=\"max\", verbose=True\n",
    ")\n",
    "early_stopping_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor=\"iou_epoch\",\n",
    "    patience=(cloud_model.patience * 3),\n",
    "    mode=\"max\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    fast_dev_run=False,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "trainer.fit(model=cloud_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best IOU on the validation split is 0.887.\n",
    "\n",
    "If you'd like to track changes in performance more closely, you could log information about metrics across batches, epochs, and models through the [TensorBoard](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='generate-submission'></a>\n",
    "\n",
    "### Generate a submission\n",
    "\n",
    "Now that we have our trained model, we can generate a full submission. **Remember that this is a [code execution](https://www.drivendata.org/competitions/83/cloud-cover/page/412/) competition,** so you will be submitting our inference code rather than our predictions. We've already written out our key class definition to scripts in the folder `benchmark_src`, which now contains:\n",
    "\n",
    "```\n",
    "benchmark_src\n",
    "├── cloud_dataset.py\n",
    "├── cloud_model.py\n",
    "└── losses.py\n",
    "```\n",
    "\n",
    "To submit to the competition, we still need to:\n",
    "\n",
    "1. Store our trained model weights in `benchmark_src` so that they can be loaded during inference\n",
    "\n",
    "2. Write a `main.py` file that loads our model weights, generates predictions for each chip, and saves the predictions to a folder called `predictions` in the same directory as itself\n",
    "\n",
    "3. Zip the contents of `benchmark_src/` - not the directory itself - into a file called `submission.zip`. \n",
    "\n",
    "4. Upload `submission.zip` to the competition submissions page. The file will be unzipped and `main.py` will be run in a [containerized execution environment](https://github.com/drivendataorg/cloud-cover-runtime) to calculate our model's IOU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Save our model\n",
    "\n",
    "First, let's make a folder for our model assets, and save the weights for our trained model using PyTorch's handy `model.save()` method. The below saves the weights to `benchmark_src/assets/cloud_model.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "submission_assets_dir = submission_dir / \"assets\"\n",
    "submission_assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_weight_path = submission_assets_dir / \"cloud_model.pt\"\n",
    "torch.save(cloud_model.state_dict(), model_weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Write `main.py`\n",
    "\n",
    "Now we'll write out a script called `main.py` to `benchmark_src`, which runs the whole inference process using the saved model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file benchmark_src/main.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import typer\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from cloud_model import CloudModel\n",
    "except ImportError:\n",
    "    from benchmark_src.cloud_dataset import CloudDataset\n",
    "    from benchmark_src.cloud_model import CloudModel\n",
    "\n",
    "\n",
    "ROOT_DIRECTORY = Path(\"/codeexecution\")\n",
    "PREDICTIONS_DIRECTORY = ROOT_DIRECTORY / \"predictions\"\n",
    "ASSETS_DIRECTORY = ROOT_DIRECTORY / \"assets\"\n",
    "DATA_DIRECTORY = ROOT_DIRECTORY / \"data\"\n",
    "INPUT_IMAGES_DIRECTORY = DATA_DIRECTORY / \"test_features\"\n",
    "\n",
    "# Set the pytorch cache directory and include cached models in your submission.zip\n",
    "os.environ[\"TORCH_HOME\"] = str(ASSETS_DIRECTORY / \"assets/torch\")\n",
    "\n",
    "\n",
    "def get_metadata(features_dir: os.PathLike, bands: List[str]):\n",
    "    \"\"\"\n",
    "    Given a folder of feature data, return a dataframe where the index is the chip id\n",
    "    and there is a column for the path to each band's TIF image.\n",
    "\n",
    "    Args:\n",
    "        features_dir (os.PathLike): path to the directory of feature data, which should have\n",
    "            a folder for each chip\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "    \"\"\"\n",
    "    chip_metadata = pd.DataFrame(index=[f\"{band}_path\" for band in bands])\n",
    "    chip_ids = (\n",
    "        pth.name for pth in features_dir.iterdir() if not pth.name.startswith(\".\")\n",
    "    )\n",
    "\n",
    "    for chip_id in chip_ids:\n",
    "        chip_bands = [features_dir / chip_id / f\"{band}.tif\" for band in bands]\n",
    "        chip_metadata[chip_id] = chip_bands\n",
    "\n",
    "    return chip_metadata.transpose().reset_index().rename(columns={\"index\": \"chip_id\"})\n",
    "\n",
    "\n",
    "def make_predictions(\n",
    "    model: CloudModel,\n",
    "    x_paths: pd.DataFrame,\n",
    "    bands: List[str],\n",
    "    predictions_dir: os.PathLike,\n",
    "):\n",
    "    \"\"\"Predicts cloud cover and saves results to the predictions directory.\n",
    "\n",
    "    Args:\n",
    "        model (CloudModel): an instantiated CloudModel based on pl.LightningModule\n",
    "        x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands provided\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "        predictions_dir (os.PathLike): Destination directory to save the predicted TIF masks\n",
    "    \"\"\"\n",
    "    test_dataset = CloudDataset(x_paths=x_paths, bands=bands)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=model.batch_size,\n",
    "        num_workers=model.num_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    for batch_index, batch in enumerate(test_dataloader):\n",
    "        logger.debug(f\"Predicting batch {batch_index} of {len(test_dataloader)}\")\n",
    "        x = batch[\"chip\"]\n",
    "        preds = model.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5).detach().numpy().astype(\"uint8\")\n",
    "        for chip_id, pred in zip(batch[\"chip_id\"], preds):\n",
    "            chip_pred_path = predictions_dir / f\"{chip_id}.tif\"\n",
    "            chip_pred_im = Image.fromarray(pred)\n",
    "            chip_pred_im.save(chip_pred_path)\n",
    "\n",
    "\n",
    "def main(\n",
    "    model_weights_path: Path = ASSETS_DIRECTORY / \"cloud_model.pt\",\n",
    "    test_features_dir: Path = DATA_DIRECTORY / \"test_features\",\n",
    "    predictions_dir: Path = PREDICTIONS_DIRECTORY,\n",
    "    bands: List[str] = [\"B02\", \"B03\", \"B04\", \"B08\"],\n",
    "    fast_dev_run: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate predictions for the chips in test_features_dir using the model saved at\n",
    "    model_weights_path.\n",
    "\n",
    "    Predictions are saved in predictions_dir. The default paths to all three files are based on\n",
    "    the structure of the code execution runtime.\n",
    "\n",
    "    Args:\n",
    "        model_weights_path (os.PathLike): Path to the weights of a trained CloudModel.\n",
    "        test_features_dir (os.PathLike, optional): Path to the features for the test data. Defaults\n",
    "            to 'data/test_features' in the same directory as main.py\n",
    "        predictions_dir (os.PathLike, optional): Destination directory to save the predicted TIF masks\n",
    "            Defaults to 'predictions' in the same directory as main.py\n",
    "        bands (List[str], optional): List of bands provided for each chip\n",
    "    \"\"\"\n",
    "    if not test_features_dir.exists():\n",
    "        raise ValueError(\n",
    "            f\"The directory for test feature images must exist and {test_features_dir} does not exist\"\n",
    "        )\n",
    "    predictions_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    logger.info(\"Loading model\")\n",
    "    model = CloudModel(bands=bands, hparams={\"weights\": None})\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "    logger.info(\"Loading test metadata\")\n",
    "    test_metadata = get_metadata(test_features_dir, bands=bands)\n",
    "    if fast_dev_run:\n",
    "        test_metadata = test_metadata.head()\n",
    "    logger.info(f\"Found {len(test_metadata)} chips\")\n",
    "\n",
    "    logger.info(\"Generating predictions in batches\")\n",
    "    make_predictions(model, test_metadata, bands, predictions_dir)\n",
    "\n",
    "    logger.info(f\"\"\"Saved {len(list(predictions_dir.glob(\"*.tif\")))} predictions\"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    typer.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to test out running `main` from this notebook, we could execute:\n",
    "\n",
    "```python\n",
    "from benchmark_src.main import main\n",
    "\n",
    "main(\n",
    "    model_weights_path=submission_dir / \"assets/cloud_model.pt\",\n",
    "    test_features_dir=TRAIN_FEATURES,\n",
    "    predictions_dir=submission_dir / \"predictions\",\n",
    "    fast_dev_run=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Zip submission contents\n",
    "\n",
    "Compress all of the submission files in `benchmark_src` into a .zip called `submission.zip`. Our final submission directory has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out our pycache before zipping up submission\n",
    "!rm -rf benchmark_src/__pycache__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree benchmark_src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to make sure that your submission does *not* include any prediction files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip submission\n",
    "!cd benchmark_src && zip -r ../submission.zip *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h submission.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload submission"
   ]
  },
  {
   "attachments": {
    "4bc76ab5-aa2c-4eab-971d-e398722f30bd.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now head over to the competition [submissions page](https://www.drivendata.org/competitions/83/cloud-cover/submissions/) to upload our code and get our model's IOU!\n",
    "\n",
    "![image.png](attachment:4bc76ab5-aa2c-4eab-971d-e398722f30bd.png)\n",
    "\n",
    "Our submission took about 20 minutes to execute. You can monitor progress during scoring with the Code Execution Status [tab](https://www.drivendata.org/competitions/83/submissions/code/). Finally, we see that we got an IOU of **0.817** - that's pretty good! It means that 81.7% of the area covered by either the ground truth labels or our predictions was shared between the two.\n",
    "\n",
    "There is still plenty of room for improvement! Head over to the On Cloud N challenge [homepage](https://www.drivendata.org/competitions/83/cloud-cover/page/396/) to get started on your own model. We're excited to see what you create!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
