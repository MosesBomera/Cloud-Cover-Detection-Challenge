{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCDC - Submission Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas_path pytorch_lightning cloudpathlib loguru typer\n",
    "!pip install --upgrade pandas==1.2.4\n",
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import rasterio\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_path import path  # noqa\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/driven-data/cloud-cover\")\n",
    "TRAIN_FEATURES = DATA_DIR / \"train_features\"\n",
    "TRAIN_LABELS = DATA_DIR / \"train_labels\"\n",
    "\n",
    "assert TRAIN_FEATURES.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS = [\"B02\", \"B03\", \"B04\", \"B08\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adwz</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id  location              datetime                   cloudpath\n",
       "0    adwp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp\n",
       "1    adwu  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu\n",
       "2    adwz  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwz\n",
       "3    adxp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp\n",
       "4    aeaj  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta = pd.read_csv(DATA_DIR / \"train_metadata.csv\")\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chip_id     11748\n",
       "location       81\n",
       "datetime       91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many different chip ids, locations, and datetimes are there?\n",
    "train_meta[[\"chip_id\", \"location\", \"datetime\"]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location_counts = (\n",
    "    train_meta.groupby(\"location\")[\"chip_id\"].nunique().sort_values(ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>10407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      chip_count\n",
       "year            \n",
       "2018         326\n",
       "2019        1015\n",
       "2020       10407"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta[\"datetime\"] = pd.to_datetime(train_meta[\"datetime\"])\n",
    "train_meta[\"year\"] = train_meta.datetime.dt.year\n",
    "train_meta.groupby(\"year\")[[\"chip_id\"]].nunique().sort_index().rename(\n",
    "    columns={\"chip_id\": \"chip_count\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2018-03-07 08:46:02+0000', tz='UTC'),\n",
       " Timestamp('2020-09-14 08:28:49+0000', tz='UTC'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta[\"datetime\"].min(), train_meta[\"datetime\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>chip_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>South America - Brazil</th>\n",
       "      <th>2020-09-06 15:02:37+00:00</th>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Port Gentil</th>\n",
       "      <th>2020-09-08 09:50:58+00:00</th>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uganda</th>\n",
       "      <th>2019-04-25 08:29:37+00:00</th>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Australia - Central</th>\n",
       "      <th>2020-08-11 01:24:00+00:00</th>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malabo</th>\n",
       "      <th>2020-09-06 10:00:03+00:00</th>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jimma</th>\n",
       "      <th>2020-05-31 08:07:58+00:00</th>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chifunfu</th>\n",
       "      <th>2020-04-29 08:20:47+00:00</th>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South America - Suriname</th>\n",
       "      <th>2020-06-03 14:11:18+00:00</th>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Isiro</th>\n",
       "      <th>2020-08-28 08:39:29+00:00</th>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pibor</th>\n",
       "      <th>2020-08-17 08:18:22+00:00</th>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    chip_count\n",
       "location                 datetime                             \n",
       "South America - Brazil   2020-09-06 15:02:37+00:00         261\n",
       "Port Gentil              2020-09-08 09:50:58+00:00         223\n",
       "Uganda                   2019-04-25 08:29:37+00:00         220\n",
       "Australia - Central      2020-08-11 01:24:00+00:00         209\n",
       "Malabo                   2020-09-06 10:00:03+00:00         206\n",
       "Jimma                    2020-05-31 08:07:58+00:00         201\n",
       "Chifunfu                 2020-04-29 08:20:47+00:00         197\n",
       "South America - Suriname 2020-06-03 14:11:18+00:00         197\n",
       "Isiro                    2020-08-28 08:39:29+00:00         197\n",
       "Pibor                    2020-08-17 08:18:22+00:00         197"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chips_per_locationtime = (\n",
    "    train_meta.groupby([\"location\", \"datetime\"])[[\"chip_id\"]]\n",
    "    .nunique()\n",
    "    .sort_values(by=\"chip_id\", ascending=False)\n",
    "    .rename(columns={\"chip_id\": \"chip_count\"})\n",
    ")\n",
    "chips_per_locationtime.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "      <th>year</th>\n",
       "      <th>B02_path</th>\n",
       "      <th>B03_path</th>\n",
       "      <th>B04_path</th>\n",
       "      <th>B08_path</th>\n",
       "      <th>label_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29 08:20:47+00:00</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "      <td>2020</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwp.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29 08:20:47+00:00</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "      <td>2020</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwu.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adwz</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29 08:20:47+00:00</td>\n",
       "      <td>az://./train_features/adwz</td>\n",
       "      <td>2020</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwz.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id  location                  datetime                   cloudpath  \\\n",
       "0    adwp  Chifunfu 2020-04-29 08:20:47+00:00  az://./train_features/adwp   \n",
       "1    adwu  Chifunfu 2020-04-29 08:20:47+00:00  az://./train_features/adwu   \n",
       "2    adwz  Chifunfu 2020-04-29 08:20:47+00:00  az://./train_features/adwz   \n",
       "\n",
       "   year                                           B02_path  \\\n",
       "0  2020  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  2020  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  2020  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "\n",
       "                                            B03_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "\n",
       "                                            B04_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "\n",
       "                                            B08_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "\n",
       "                                       label_path  \n",
       "0  /driven-data/cloud-cover/train_labels/adwp.tif  \n",
       "1  /driven-data/cloud-cover/train_labels/adwu.tif  \n",
       "2  /driven-data/cloud-cover/train_labels/adwz.tif  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_paths(df, feature_dir, label_dir=None, bands=BANDS):\n",
    "    \"\"\"\n",
    "    Given dataframe with a column for chip_id, returns a dataframe with a column\n",
    "    added indicating the path to each band's TIF image as \"{band}_path\", eg \"B02_path\".\n",
    "    A column is also added to the dataframe with paths to the label TIF, if the\n",
    "    path to the labels directory is provided.\n",
    "    \"\"\"\n",
    "    for band in bands:\n",
    "        df[f\"{band}_path\"] = feature_dir / df[\"chip_id\"] / f\"{band}.tif\"\n",
    "        # make sure a random sample of paths exist\n",
    "        assert df.sample(n=40, random_state=5)[f\"{band}_path\"].path.exists().all()\n",
    "    if label_dir is not None:\n",
    "        df[\"label_path\"] = label_dir / (df[\"chip_id\"] + \".tif\")\n",
    "        # make sure a random sample of paths exist\n",
    "        assert df.sample(n=40, random_state=5)[\"label_path\"].path.exists().all()\n",
    "    return df\n",
    "\n",
    "train_meta = add_paths(train_meta, TRAIN_FEATURES, TRAIN_LABELS)\n",
    "train_meta.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_long_bounds(filepath):\n",
    "    \"\"\"Given the path to a GeoTIFF, returns the image bounds in latitude and\n",
    "    longitude coordinates.\n",
    "\n",
    "    Returns points as a tuple of (left, bottom, right, top)\n",
    "    \"\"\"\n",
    "    with rasterio.open(filepath) as im:\n",
    "        bounds = im.bounds\n",
    "        meta = im.meta\n",
    "    # create a converter starting with the current projection\n",
    "    current_crs = pyproj.CRS(meta[\"crs\"])\n",
    "    crs_transform = pyproj.Transformer.from_crs(current_crs, current_crs.geodetic_crs)\n",
    "\n",
    "    # returns left, bottom, right, top\n",
    "    return crs_transform.transform_bounds(*bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_color_img(chip_id, data_dir=TRAIN_FEATURES):\n",
    "    \"\"\"Given the path to the directory of Sentinel-2 chip feature images,\n",
    "    plots the true color image\"\"\"\n",
    "    chip_dir = data_dir / chip_id\n",
    "    red = rioxarray.open_rasterio(chip_dir / \"B04.tif\").squeeze()\n",
    "    green = rioxarray.open_rasterio(chip_dir / \"B03.tif\").squeeze()\n",
    "    blue = rioxarray.open_rasterio(chip_dir / \"B02.tif\").squeeze()\n",
    "\n",
    "    return ms.true_color(r=red, g=green, b=blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split\n",
    "We have chosen the simplest route, and split our training chips randomly into 1/3 validation and 2/3 training. You may want to think about splitting by location instead of by chip, to better check how your model will do in new settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3877, 10), (7871, 10))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(9)  # set a seed for reproducibility\n",
    "\n",
    "# put 1/3 of chips into the validation set\n",
    "chip_ids = train_meta.chip_id.unique().tolist()\n",
    "val_chip_ids = random.sample(chip_ids, round(len(chip_ids) * 0.33))\n",
    "\n",
    "val_mask = train_meta.chip_id.isin(val_chip_ids)\n",
    "val = train_meta[val_mask].copy().reset_index(drop=True)\n",
    "train = train_meta[~val_mask].copy().reset_index(drop=True)\n",
    "\n",
    "val.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features from labels\n",
    "feature_cols = [\"chip_id\"] + [f\"{band}_path\" for band in BANDS]\n",
    "\n",
    "val_x = val[feature_cols].copy()\n",
    "val_y = val[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "train_x = train[feature_cols].copy()\n",
    "train_y = train[[\"chip_id\", \"label_path\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>B02_path</th>\n",
       "      <th>B03_path</th>\n",
       "      <th>B04_path</th>\n",
       "      <th>B08_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwz</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aeej</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeej/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeej/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeej/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeej/B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aeey</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeey/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeey/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeey/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeey/B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aegb</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aegb/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aegb/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aegb/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aegb/B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeky</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeky/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeky/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeky/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeky/B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id                                           B02_path  \\\n",
       "0    adwz  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "1    aeej  /driven-data/cloud-cover/train_features/aeej/B...   \n",
       "2    aeey  /driven-data/cloud-cover/train_features/aeey/B...   \n",
       "3    aegb  /driven-data/cloud-cover/train_features/aegb/B...   \n",
       "4    aeky  /driven-data/cloud-cover/train_features/aeky/B...   \n",
       "\n",
       "                                            B03_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "1  /driven-data/cloud-cover/train_features/aeej/B...   \n",
       "2  /driven-data/cloud-cover/train_features/aeey/B...   \n",
       "3  /driven-data/cloud-cover/train_features/aegb/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeky/B...   \n",
       "\n",
       "                                            B04_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "1  /driven-data/cloud-cover/train_features/aeej/B...   \n",
       "2  /driven-data/cloud-cover/train_features/aeey/B...   \n",
       "3  /driven-data/cloud-cover/train_features/aegb/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeky/B...   \n",
       "\n",
       "                                            B08_path  \n",
       "0  /driven-data/cloud-cover/train_features/adwz/B...  \n",
       "1  /driven-data/cloud-cover/train_features/aeej/B...  \n",
       "2  /driven-data/cloud-cover/train_features/aeey/B...  \n",
       "3  /driven-data/cloud-cover/train_features/aegb/B...  \n",
       "4  /driven-data/cloud-cover/train_features/aeky/B...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>label_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwz</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwz.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aeej</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/aeej.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aeey</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/aeey.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aegb</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/aegb.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeky</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/aeky.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id                                      label_path\n",
       "0    adwz  /driven-data/cloud-cover/train_labels/adwz.tif\n",
       "1    aeej  /driven-data/cloud-cover/train_labels/aeej.tif\n",
       "2    aeey  /driven-data/cloud-cover/train_labels/aeey.tif\n",
       "3    aegb  /driven-data/cloud-cover/train_labels/aegb.tif\n",
       "4    aeky  /driven-data/cloud-cover/train_labels/aeky.tif"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUBMISSION BY DESIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create benchmark_src folder\n",
    "submission_dir = Path(\"train_src\")\n",
    "if submission_dir.exists():\n",
    "    shutil.rmtree(submission_dir)\n",
    "\n",
    "submission_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_src/cloud_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/cloud_dataset.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "\n",
    "\n",
    "class CloudDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Reads in images, transforms pixel values, and serves a\n",
    "    dictionary containing chip ids, image tensors, and\n",
    "    label masks (where available).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_paths: pd.DataFrame,\n",
    "        bands: List[str],\n",
    "        y_paths: Optional[pd.DataFrame] = None,\n",
    "        transforms: Optional[list] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudDataset class.\n",
    "\n",
    "        Args:\n",
    "            x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands\n",
    "            bands (list[str]): list of the bands included in the data\n",
    "            y_paths (pd.DataFrame, optional): a dataframe with a for each chip and columns for chip_id\n",
    "                and the path to the label TIF with ground truth cloud cover\n",
    "            transforms (list, optional): list of transforms to apply to the feature data (eg augmentations)\n",
    "        \"\"\"\n",
    "        self.data = x_paths\n",
    "        self.label = y_paths\n",
    "        self.transforms = transforms\n",
    "        self.bands = bands\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Loads an n-channel image from a chip-level dataframe\n",
    "        img = self.data.loc[idx]\n",
    "        band_arrs = []\n",
    "        for band in self.bands:\n",
    "            with rasterio.open(img[f\"{band}_path\"]) as b:\n",
    "                band_arr = b.read(1).astype(\"float32\")\n",
    "            band_arrs.append(band_arr)\n",
    "        x_arr = np.stack(band_arrs, axis=-1)\n",
    "\n",
    "        # Apply data augmentations, if provided\n",
    "        if self.transforms:\n",
    "            x_arr = self.transforms(image=x_arr)[\"image\"]\n",
    "        x_arr = np.transpose(x_arr, [2, 0, 1])\n",
    "\n",
    "        # Prepare dictionary for item\n",
    "        item = {\"chip_id\": img.chip_id, \"chip\": x_arr}\n",
    "\n",
    "        # Load label if available\n",
    "        if self.label is not None:\n",
    "            label_path = self.label.loc[idx].label_path\n",
    "            with rasterio.open(label_path) as lp:\n",
    "                y_arr = lp.read(1).astype(\"float32\")\n",
    "            # Apply same data augmentations to the label\n",
    "            if self.transforms:\n",
    "                y_arr = self.transforms(image=y_arr)[\"image\"]\n",
    "            item[\"label\"] = y_arr\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_src/losses.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/losses.py\n",
    "import numpy as np\n",
    "\n",
    "def intersection_over_union(pred, true):\n",
    "    \"\"\"\n",
    "    Calculates intersection and union for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): a tensor of predictions\n",
    "        true (torc.Tensor): a tensor of labels\n",
    "\n",
    "    Returns:\n",
    "        intersection (int): total intersection of pixels\n",
    "        union (int): total union of pixels\n",
    "    \"\"\"\n",
    "    valid_pixel_mask = true.ne(255)  # valid pixel mask\n",
    "    true = true.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "    pred = pred.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "\n",
    "    # Intersection and union totals\n",
    "    intersection = np.logical_and(true, pred)\n",
    "    union = np.logical_or(true, pred)\n",
    "    return intersection.sum() / union.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CloudModel`\n",
    "\n",
    "Now is the moment we've all been waiting for - coding our actual model! \n",
    "\n",
    "Again, we'll make our lives simpler by starting with the [`pl.LightningModule`](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html) class from Pytorch Lightning. This comes with most of the logic we need, so we only have to specify components that are specific to our modeling setup. Our custom `CloudModel` class will define:\n",
    "\n",
    "- `__init__`: how to instantiate a `CloudModel` class\n",
    "\n",
    "- `forward`: forward pass for an image in the neural network propogation\n",
    "\n",
    "- `training_step`: switch the model to train mode, implement the forward pass, and calculate training loss (cross-entropy) for a batch\n",
    "\n",
    "- `validation_step`: switch the model to eval mode and calculate validation loss (IOU) for the batch\n",
    "\n",
    "- `train_dataloader`: call an iterable over the training dataset for automatic batching\n",
    "\n",
    "- `val_dataloader`: call an iterable over the validation dataset for automatic batching\n",
    "\n",
    "- `configure_optimizers`: configure an [optimizer](https://pytorch.org/docs/stable/optim.html) and a [scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html) to dynamically adjust the learning rate based on the number of epochs\n",
    "\n",
    "- `_prepare_model`: load the U-Net model with a ResNet34 backbone from the `segmentation_models_pytorch` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL VERSIONING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| No | Model | Backbone | Weights | Folder | Accuracy |\n",
    "| --- | --- | --- | -- | -- | -- | \n",
    "| 1 | unet | resnet35 | imagenet | version_3/checkpoints/epoch=25-step=25583.ckpt | .877 |\n",
    "| 2 | unet | resnet50 | imagenet | lightning_logs/version_9/checkpoints/epoch=33-step=33455.ckpt| .882 |\n",
    "| 3 | unet | resnet101 | imagenet | version_12/checkpoints/epoch=8-step=8855.ckpt | .878 |\n",
    "| 4 | unet | resnet50 | imagenet - DiceLoss + 0.5TverskyFocal | version_13/checkpoints/epoch=12-step=12791.ckpt | .880 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_transformations = albumentations.Compose(\n",
    "#     [\n",
    "#         albumentations.RandomCrop(256, 256),\n",
    "#         albumentations.RandomRotate90(),\n",
    "#         albumentations.HorizontalFlip(),\n",
    "#         albumentations.VerticalFlip(),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_src/cloud_model.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/cloud_model.py\n",
    "from typing import Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch.losses as Loss\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from losses import intersection_over_union\n",
    "except ImportError:\n",
    "    from train_src.cloud_dataset import CloudDataset\n",
    "    from train_src.losses import intersection_over_union\n",
    "\n",
    "class CloudModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bands: List[str],\n",
    "        x_train: Optional[pd.DataFrame] = None,\n",
    "        y_train: Optional[pd.DataFrame] = None,\n",
    "        x_val: Optional[pd.DataFrame] = None,\n",
    "        y_val: Optional[pd.DataFrame] = None,\n",
    "        hparams: dict = {},\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudModel class based on the pl.LightningModule\n",
    "        (https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\n",
    "\n",
    "        Args:\n",
    "            bands (list[str]): Names of the bands provided for each chip\n",
    "            x_train (pd.DataFrame, optional): a dataframe of the training features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_train (pd.DataFrame, optional): a dataframe of the training labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            x_val (pd.DataFrame, optional): a dataframe of the validation features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_val (pd.DataFrame, optional): a dataframe of the validation labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            hparams (dict, optional): Dictionary of additional modeling parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # required\n",
    "        self.bands = bands\n",
    "\n",
    "        # optional modeling params\n",
    "        self.backbone = self.hparams.get(\"backbone\", \"resnet50\")\n",
    "        self.weights = self.hparams.get(\"weights\", \"imagenet\")\n",
    "        self.learning_rate = self.hparams.get(\"lr\", 1e-3)\n",
    "        self.transforms = self.hparams.get(\"transforms\", None)\n",
    "        self.patience = self.hparams.get(\"patience\", 4)\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 2)\n",
    "        self.batch_size = self.hparams.get(\"batch_size\", 32)\n",
    "        self.gpu = self.hparams.get(\"gpu\", False)\n",
    "\n",
    "        # Instantiate datasets, model, and trainer params if provided\n",
    "        self.train_dataset = CloudDataset(\n",
    "            x_paths=x_train,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_train,\n",
    "            transforms=self.transforms,\n",
    "        )\n",
    "        self.val_dataset = CloudDataset(\n",
    "            x_paths=x_val,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_val,\n",
    "            transforms=None,\n",
    "        )\n",
    "        self.model = self._prepare_model()\n",
    "\n",
    "    ## Required LightningModule methods ##\n",
    "\n",
    "    def forward(self, image: torch.Tensor):\n",
    "        # Forward pass\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.train_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_train and y_train must be specified when CloudModel is instantiated to run training\"\n",
    "            )\n",
    "\n",
    "        # Switch on training mode\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        # Change to int.\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = self.forward(x)\n",
    "\n",
    "        # Log batch loss\n",
    "        # Combined loss.\n",
    "        loss = Loss.DiceLoss(mode='multiclass', smooth=1.0)(preds, y).mean() \\\n",
    "               + 0.5 * Loss.TverskyLoss(mode='multiclass')(preds, y).mean()\n",
    "        \n",
    "        self.log(\n",
    "            \"loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.val_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_val and y_val must be specified when CloudModel is instantiated to run validation\"\n",
    "            )\n",
    "\n",
    "        # Switch on validation mode\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass & softmax\n",
    "        preds = self.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5) * 1  # convert to int\n",
    "\n",
    "        # Log batch IOU\n",
    "        batch_iou = intersection_over_union(preds, y)\n",
    "        self.log(\n",
    "            \"iou\", batch_iou, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return batch_iou\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # DataLoader class for training\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # DataLoader class for validation\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "        return [opt], [sch]\n",
    "\n",
    "    ## Convenience Methods ##\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        # Instantiate U-Net model\n",
    "        unet_model = smp.Unet(\n",
    "            encoder_name=self.backbone,\n",
    "            encoder_weights=self.weights,\n",
    "            in_channels=4,\n",
    "            classes=2,\n",
    "        )\n",
    "        if self.gpu:\n",
    "            unet_model.cuda()\n",
    "\n",
    "        return unet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_src.cloud_model import CloudModel\n",
    "\n",
    "# Set up pytorch_lightning.Trainer object\n",
    "cloud_model = CloudModel(\n",
    "    bands=BANDS,\n",
    "    x_train=train_x,\n",
    "    y_train=train_y,\n",
    "    x_val=val_x,\n",
    "    y_val=val_y,\n",
    "    hparams={\n",
    "        \"num_workers\": 4, \n",
    "        \"batch_size\": 8,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"iou_epoch\", mode=\"max\", verbose=True,\n",
    ")\n",
    "early_stopping_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor=\"iou_epoch\",\n",
    "    patience=(cloud_model.patience * 3),\n",
    "    mode=\"max\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    fast_dev_run=False,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | Unet | 32.5 M\n",
      "-------------------------------\n",
      "32.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.5 M    Total params\n",
      "130.098   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4e5cb4aae44377835c79c0548c5287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-4, 3], but got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcloud_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    735\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m     )\n\u001b[1;32m    739\u001b[0m     train_dataloaders \u001b[38;5;241m=\u001b[39m train_dataloader\n\u001b[0;32m--> 740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;66;03m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    776\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;66;03m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mstart_predicting(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1319\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:234\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m data_fetcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mget_profiled_dataloader(dataloader)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# as they expect that the same step is used when logging epoch end metrics even when the batch loop has\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# finished. this means the attribute does not exactly track the number of optimizer steps applied.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# TODO(@carmocca): deprecate and rename so users don't get confused\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:193\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 193\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m     87\u001b[0m     optimizers \u001b[38;5;241m=\u001b[39m _get_active_optimizers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizer_frequencies, batch_idx)\n\u001b[0;32m---> 88\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_loop\u001b[38;5;241m.\u001b[39mrun(split_batch, batch_idx)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:215\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madvance\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_optimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim_progress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_position\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;66;03m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;66;03m# would be skipped otherwise\u001b[39;00m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_idx] \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39masdict()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:266\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[1;32m    259\u001b[0m         closure()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:378\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_tpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDeviceType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTPU\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_TPU_AVAILABLE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_native_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mAMPType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNATIVE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_lbfgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_lbfgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1652\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1573\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     using_lbfgs: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1581\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;124;03m    Override this method to adjust the default way the\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;124;03m    :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \n\u001b[1;32m   1651\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:164\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m trainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(profiler_action):\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:336\u001b[0m, in \u001b[0;36mAccelerator.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m\"\"\"performs the actual optimizer step.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    **kwargs: Any extra arguments to ``optimizer.step``\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    335\u001b[0m model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:163\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m    162\u001b[0m     closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> 163\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/optim/adam.py:92\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m---> 92\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m     95\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:148\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:160\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:142\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step_and_backward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m         step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    146\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m             )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:435\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx)\u001b[0m\n\u001b[1;32m    433\u001b[0m lightning_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 435\u001b[0m     training_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mpost_training_step()\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m step_kwargs\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:216\u001b[0m, in \u001b[0;36mAccelerator.training_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m\"\"\"The actual training step.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.training_step` for more details\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:213\u001b[0m, in \u001b[0;36mTrainingTypePlugin.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PlanetaryComputerExamples/competitions/cloud-cover/train_src/cloud_model.py:114\u001b[0m, in \u001b[0;36mCloudModel.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    110\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Log batch loss\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Combined loss.\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mLoss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoftCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean() \\\n\u001b[1;32m    115\u001b[0m        \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m Loss\u001b[38;5;241m.\u001b[39mTverskyLoss(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m'\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)(preds, y)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    119\u001b[0m     loss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m     logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    124\u001b[0m )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/segmentation_models_pytorch/losses/soft_ce.py:40\u001b[0m, in \u001b[0;36mSoftCrossEntropyLoss.forward\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_pred: torch\u001b[38;5;241m.\u001b[39mTensor, y_true: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 40\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m label_smoothed_nll_loss(\n\u001b[1;32m     42\u001b[0m         log_prob,\n\u001b[1;32m     43\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim,\n\u001b[1;32m     48\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/nn/functional.py:1769\u001b[0m, in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1767\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1769\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1771\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mlog_softmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-4, 3], but got 4)"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "trainer.fit(model=cloud_model, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last checkpoint, noted - 0.87612"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best IOU on the validation split is 0.887.\n",
    "\n",
    "If you'd like to track changes in performance more closely, you could log information about metrics across batches, epochs, and models through the [TensorBoard](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='generate-submission'></a>\n",
    "\n",
    "### Generate a submission\n",
    "\n",
    "Now that we have our trained model, we can generate a full submission. **Remember that this is a [code execution](https://www.drivendata.org/competitions/83/cloud-cover/page/412/) competition,** so you will be submitting our inference code rather than our predictions. We've already written out our key class definition to scripts in the folder `benchmark_src`, which now contains:\n",
    "\n",
    "```\n",
    "benchmark_src\n",
    " cloud_dataset.py\n",
    " cloud_model.py\n",
    " losses.py\n",
    "```\n",
    "\n",
    "To submit to the competition, we still need to:\n",
    "\n",
    "1. Store our trained model weights in `benchmark_src` so that they can be loaded during inference\n",
    "\n",
    "2. Write a `main.py` file that loads our model weights, generates predictions for each chip, and saves the predictions to a folder called `predictions` in the same directory as itself\n",
    "\n",
    "3. Zip the contents of `benchmark_src/` - not the directory itself - into a file called `submission.zip`. \n",
    "\n",
    "4. Upload `submission.zip` to the competition submissions page. The file will be unzipped and `main.py` will be run in a [containerized execution environment](https://github.com/drivendataorg/cloud-cover-runtime) to calculate our model's IOU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Save our model\n",
    "\n",
    "First, let's make a folder for our model assets, and save the weights for our trained model using PyTorch's handy `model.save()` method. The below saves the weights to `benchmark_src/assets/cloud_model.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "submission_assets_dir = submission_dir / \"assets\"\n",
    "submission_assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_weight_path = submission_assets_dir / \"cloud_model.pt\"\n",
    "torch.save(cloud_model.state_dict(), model_weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Write `main.py`\n",
    "\n",
    "Now we'll write out a script called `main.py` to `benchmark_src`, which runs the whole inference process using the saved model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file benchmark_src/main.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import typer\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from cloud_model import CloudModel\n",
    "except ImportError:\n",
    "    from benchmark_src.cloud_dataset import CloudDataset\n",
    "    from benchmark_src.cloud_model import CloudModel\n",
    "\n",
    "\n",
    "ROOT_DIRECTORY = Path(\"/codeexecution\")\n",
    "PREDICTIONS_DIRECTORY = ROOT_DIRECTORY / \"predictions\"\n",
    "ASSETS_DIRECTORY = ROOT_DIRECTORY / \"assets\"\n",
    "DATA_DIRECTORY = ROOT_DIRECTORY / \"data\"\n",
    "INPUT_IMAGES_DIRECTORY = DATA_DIRECTORY / \"test_features\"\n",
    "\n",
    "# Set the pytorch cache directory and include cached models in your submission.zip\n",
    "os.environ[\"TORCH_HOME\"] = str(ASSETS_DIRECTORY / \"assets/torch\")\n",
    "\n",
    "\n",
    "def get_metadata(features_dir: os.PathLike, bands: List[str]):\n",
    "    \"\"\"\n",
    "    Given a folder of feature data, return a dataframe where the index is the chip id\n",
    "    and there is a column for the path to each band's TIF image.\n",
    "\n",
    "    Args:\n",
    "        features_dir (os.PathLike): path to the directory of feature data, which should have\n",
    "            a folder for each chip\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "    \"\"\"\n",
    "    chip_metadata = pd.DataFrame(index=[f\"{band}_path\" for band in bands])\n",
    "    chip_ids = (\n",
    "        pth.name for pth in features_dir.iterdir() if not pth.name.startswith(\".\")\n",
    "    )\n",
    "\n",
    "    for chip_id in chip_ids:\n",
    "        chip_bands = [features_dir / chip_id / f\"{band}.tif\" for band in bands]\n",
    "        chip_metadata[chip_id] = chip_bands\n",
    "\n",
    "    return chip_metadata.transpose().reset_index().rename(columns={\"index\": \"chip_id\"})\n",
    "\n",
    "\n",
    "def make_predictions(\n",
    "    model: CloudModel,\n",
    "    x_paths: pd.DataFrame,\n",
    "    bands: List[str],\n",
    "    predictions_dir: os.PathLike,\n",
    "):\n",
    "    \"\"\"Predicts cloud cover and saves results to the predictions directory.\n",
    "\n",
    "    Args:\n",
    "        model (CloudModel): an instantiated CloudModel based on pl.LightningModule\n",
    "        x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands provided\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "        predictions_dir (os.PathLike): Destination directory to save the predicted TIF masks\n",
    "    \"\"\"\n",
    "    test_dataset = CloudDataset(x_paths=x_paths, bands=bands)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=model.batch_size,\n",
    "        num_workers=model.num_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    for batch_index, batch in enumerate(test_dataloader):\n",
    "        logger.debug(f\"Predicting batch {batch_index} of {len(test_dataloader)}\")\n",
    "        x = batch[\"chip\"]\n",
    "        preds = model.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5).detach().numpy().astype(\"uint8\")\n",
    "        for chip_id, pred in zip(batch[\"chip_id\"], preds):\n",
    "            chip_pred_path = predictions_dir / f\"{chip_id}.tif\"\n",
    "            chip_pred_im = Image.fromarray(pred)\n",
    "            chip_pred_im.save(chip_pred_path)\n",
    "\n",
    "\n",
    "def main(\n",
    "    model_weights_path: Path = ASSETS_DIRECTORY / \"cloud_model.pt\",\n",
    "    test_features_dir: Path = DATA_DIRECTORY / \"test_features\",\n",
    "    predictions_dir: Path = PREDICTIONS_DIRECTORY,\n",
    "    bands: List[str] = [\"B02\", \"B03\", \"B04\", \"B08\"],\n",
    "    fast_dev_run: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate predictions for the chips in test_features_dir using the model saved at\n",
    "    model_weights_path.\n",
    "\n",
    "    Predictions are saved in predictions_dir. The default paths to all three files are based on\n",
    "    the structure of the code execution runtime.\n",
    "\n",
    "    Args:\n",
    "        model_weights_path (os.PathLike): Path to the weights of a trained CloudModel.\n",
    "        test_features_dir (os.PathLike, optional): Path to the features for the test data. Defaults\n",
    "            to 'data/test_features' in the same directory as main.py\n",
    "        predictions_dir (os.PathLike, optional): Destination directory to save the predicted TIF masks\n",
    "            Defaults to 'predictions' in the same directory as main.py\n",
    "        bands (List[str], optional): List of bands provided for each chip\n",
    "    \"\"\"\n",
    "    if not test_features_dir.exists():\n",
    "        raise ValueError(\n",
    "            f\"The directory for test feature images must exist and {test_features_dir} does not exist\"\n",
    "        )\n",
    "    predictions_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    logger.info(\"Loading model\")\n",
    "    model = CloudModel(bands=bands, hparams={\"weights\": None})\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "    logger.info(\"Loading test metadata\")\n",
    "    test_metadata = get_metadata(test_features_dir, bands=bands)\n",
    "    if fast_dev_run:\n",
    "        test_metadata = test_metadata.head()\n",
    "    logger.info(f\"Found {len(test_metadata)} chips\")\n",
    "\n",
    "    logger.info(\"Generating predictions in batches\")\n",
    "    make_predictions(model, test_metadata, bands, predictions_dir)\n",
    "\n",
    "    logger.info(f\"\"\"Saved {len(list(predictions_dir.glob(\"*.tif\")))} predictions\"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    typer.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to test out running `main` from this notebook, we could execute:\n",
    "\n",
    "```python\n",
    "from benchmark_src.main import main\n",
    "\n",
    "main(\n",
    "    model_weights_path=submission_dir / \"assets/cloud_model.pt\",\n",
    "    test_features_dir=TRAIN_FEATURES,\n",
    "    predictions_dir=submission_dir / \"predictions\",\n",
    "    fast_dev_run=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Zip submission contents\n",
    "\n",
    "Compress all of the submission files in `benchmark_src` into a .zip called `submission.zip`. Our final submission directory has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out our pycache before zipping up submission\n",
    "!rm -rf benchmark_src/__pycache__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree benchmark_src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to make sure that your submission does *not* include any prediction files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip submission\n",
    "!cd benchmark_src && zip -r ../submission.zip *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h submission.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload submission"
   ]
  },
  {
   "attachments": {
    "4bc76ab5-aa2c-4eab-971d-e398722f30bd.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now head over to the competition [submissions page](https://www.drivendata.org/competitions/83/cloud-cover/submissions/) to upload our code and get our model's IOU!\n",
    "\n",
    "![image.png](attachment:4bc76ab5-aa2c-4eab-971d-e398722f30bd.png)\n",
    "\n",
    "Our submission took about 20 minutes to execute. You can monitor progress during scoring with the Code Execution Status [tab](https://www.drivendata.org/competitions/83/submissions/code/). Finally, we see that we got an IOU of **0.817** - that's pretty good! It means that 81.7% of the area covered by either the ground truth labels or our predictions was shared between the two.\n",
    "\n",
    "There is still plenty of room for improvement! Head over to the On Cloud N challenge [homepage](https://www.drivendata.org/competitions/83/cloud-cover/page/396/) to get started on your own model. We're excited to see what you create!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
